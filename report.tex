\documentclass[12pt,a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Transformer模型实现报告}
\author{大模型课程作业}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{引言}

Transformer模型是Vaswani等人在2017年提出的革命性架构，它完全基于注意力机制，摒弃了传统的循环和卷积结构。Transformer在机器翻译、文本生成、语言理解等任务上取得了突破性的成果，成为了现代大语言模型的基础架构。

本报告详细介绍了Transformer模型的数学原理、实现细节和实验结果。我们手动实现了Transformer的核心组件，包括多头自注意力机制、位置前馈网络、残差连接和层归一化，并在小规模文本建模任务上进行了训练和验证。

\section{相关工作}

\subsection{注意力机制的发展}

注意力机制最早在序列到序列模型中引入，用于解决长序列建模问题。Bahdanau等人提出了注意力机制，允许模型在生成每个输出时关注输入序列的不同部分。随后，Luong等人提出了多种注意力变体。

\subsection{Transformer架构}

Transformer架构的核心创新在于：
\begin{itemize}
    \item 完全基于注意力机制，无需循环或卷积
    \item 多头注意力机制，允许模型同时关注不同表示子空间
    \item 位置编码，为序列提供位置信息
    \item 残差连接和层归一化，使深层网络训练更稳定
\end{itemize}

\subsection{后续发展}

基于Transformer架构，后续出现了BERT、GPT、T5等预训练模型，这些模型在各种NLP任务上取得了显著成果。

\section{数学推导}

\subsection{缩放点积注意力}

注意力机制的核心是计算查询（Query）、键（Key）和值（Value）之间的关系。缩放点积注意力的计算公式为：

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

其中：
\begin{itemize}
    \item $Q \in \mathbb{R}^{n \times d_k}$ 是查询矩阵
    \item $K \in \mathbb{R}^{m \times d_k}$ 是键矩阵
    \item $V \in \mathbb{R}^{m \times d_v}$ 是值矩阵
    \item $d_k$ 是键的维度
    \item $\sqrt{d_k}$ 是缩放因子，防止点积值过大导致softmax梯度消失
\end{itemize}

\subsection{多头注意力}

多头注意力允许模型同时关注不同的表示子空间：

\begin{equation}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\end{equation}

其中每个注意力头为：

\begin{equation}
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

参数矩阵：
\begin{itemize}
    \item $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$
    \item $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$
    \item $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$
    \item $W^O \in \mathbb{R}^{hd_v \times d_{model}}$
\end{itemize}

通常设置 $d_k = d_v = d_{model}/h$，其中 $h$ 是注意力头数。

\subsection{位置前馈网络}

位置前馈网络对每个位置独立应用相同的两层全连接网络：

\begin{equation}
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\end{equation}

或者使用GELU激活函数：

\begin{equation}
\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
\end{equation}

其中：
\begin{itemize}
    \item $W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$
    \item $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$
    \item $d_{ff}$ 是前馈网络的隐藏层维度，通常为 $4d_{model}$
\end{itemize}

\subsection{位置编码}

由于Transformer没有循环结构，需要显式地注入位置信息。我们使用正弦位置编码：

\begin{equation}
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}

\begin{equation}
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}

其中：
\begin{itemize}
    \item $pos$ 是位置索引
    \item $i$ 是维度索引
    \item $d_{model}$ 是模型维度
\end{itemize}

位置编码与词嵌入相加：

\begin{equation}
x = \text{Embedding}(token) + PE(pos)
\end{equation}

\subsection{残差连接和层归一化}

残差连接允许梯度直接传播，缓解深层网络的梯度消失问题：

\begin{equation}
x_{out} = \text{LayerNorm}(x + \text{Sublayer}(x))
\end{equation}

层归一化对每个样本的特征维度进行归一化：

\begin{equation}
\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\end{equation}

其中：
\begin{itemize}
    \item $\mu = \frac{1}{d}\sum_{i=1}^d x_i$ 是均值
    \item $\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2$ 是方差
    \item $\gamma$ 和 $\beta$ 是可学习的缩放和偏移参数
    \item $\epsilon$ 是防止除零的小常数
\end{itemize}

\subsection{编码器层}

编码器层由两个子层组成：

\begin{align}
x_1 &= \text{LayerNorm}(x + \text{MultiHeadAttention}(x, x, x)) \\
x_2 &= \text{LayerNorm}(x_1 + \text{FFN}(x_1))
\end{align}

\subsection{解码器层}

解码器层由三个子层组成：

\begin{align}
x_1 &= \text{LayerNorm}(x + \text{MaskedMultiHeadAttention}(x, x, x)) \\
x_2 &= \text{LayerNorm}(x_1 + \text{MultiHeadAttention}(x_1, \text{encoder\_output}, \text{encoder\_output})) \\
x_3 &= \text{LayerNorm}(x_2 + \text{FFN}(x_2))
\end{align}

其中第一个注意力层使用掩码，防止模型看到未来的信息。

\section{伪代码}

\subsection{多头自注意力伪代码}

\begin{algorithm}
\caption{多头自注意力}
\begin{algorithmic}[1]
\REQUIRE 查询矩阵 $Q$，键矩阵 $K$，值矩阵 $V$，头数 $h$
\ENSURE 输出 $O$ 和注意力权重 $A$
\STATE $d_k \gets \text{dim}(Q) / h$
\STATE 将 $Q, K, V$ 分割为 $h$ 个头：$Q_i, K_i, V_i$ for $i = 1, \ldots, h$
\FOR{$i = 1$ to $h$}
    \STATE $S_i \gets Q_i K_i^T / \sqrt{d_k}$ \COMMENT{计算注意力分数}
    \STATE $A_i \gets \text{softmax}(S_i)$ \COMMENT{归一化}
    \STATE $\text{head}_i \gets A_i V_i$ \COMMENT{加权求和}
\ENDFOR
\STATE $O \gets \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O$
\RETURN $O, A$
\end{algorithmic}
\end{algorithm}

\subsection{Transformer编码器层伪代码}

\begin{algorithm}
\caption{Transformer编码器层}
\begin{algorithmic}[1]
\REQUIRE 输入 $x$，掩码 $mask$
\ENSURE 输出 $x_{out}$
\STATE $x_{attn}, A \gets \text{MultiHeadAttention}(x, x, x, mask)$
\STATE $x \gets \text{LayerNorm}(x + \text{Dropout}(x_{attn}))$ \COMMENT{残差连接}
\STATE $x_{ffn} \gets \text{FFN}(x)$
\STATE $x_{out} \gets \text{LayerNorm}(x + \text{Dropout}(x_{ffn}))$ \COMMENT{残差连接}
\RETURN $x_{out}$
\end{algorithmic}
\end{algorithm}

\subsection{Transformer训练伪代码}

\begin{algorithm}
\caption{Transformer训练流程}
\begin{algorithmic}[1]
\REQUIRE 训练数据 $D_{train}$，验证数据 $D_{val}$，模型 $M$，优化器 $O$
\ENSURE 训练好的模型 $M^*$
\STATE 初始化模型参数
\FOR{epoch $= 1$ to $E$}
    \FOR{每个批次 $(x, y)$ in $D_{train}$}
        \STATE $\hat{y} \gets M(x)$ \COMMENT{前向传播}
        \STATE $\mathcal{L} \gets \text{CrossEntropy}(\hat{y}, y)$ \COMMENT{计算损失}
        \STATE $\nabla \mathcal{L} \gets \text{Backward}(\mathcal{L})$ \COMMENT{反向传播}
        \STATE $\text{clip}(\nabla \mathcal{L}, \text{max\_norm})$ \COMMENT{梯度裁剪}
        \STATE $O.\text{step}()$ \COMMENT{更新参数}
    \ENDFOR
    \STATE $\text{val\_loss} \gets \text{Evaluate}(M, D_{val})$
    \IF{$\text{val\_loss} < \text{best\_loss}$}
        \STATE $\text{Save}(M)$ \COMMENT{保存最佳模型}
    \ENDIF
    \STATE $\text{UpdateLearningRate}()$ \COMMENT{学习率调度}
\ENDFOR
\RETURN $M^*$
\end{algorithmic}
\end{algorithm}

\section{框架描述}

\subsection{整体架构}

我们的Transformer实现包含以下核心组件：

\begin{itemize}
    \item \textbf{多头自注意力模块}：实现了缩放点积注意力和多头机制
    \item \textbf{位置前馈网络}：实现了两层全连接网络
    \item \textbf{位置编码}：实现了正弦位置编码
    \item \textbf{层归一化}：实现了层归一化和残差连接
    \item \textbf{编码器层}：包含自注意力和前馈网络
    \item \textbf{解码器层}：包含掩码自注意力、交叉注意力和前馈网络
    \item \textbf{完整Transformer}：包含编码器和解码器的完整架构
    \item \textbf{TransformerLM}：用于语言建模的仅编码器版本
\end{itemize}

\subsection{数据流程}

\begin{enumerate}
    \item 文本数据经过分词和编码，转换为token ID序列
    \item Token ID通过词嵌入层转换为向量表示
    \item 添加位置编码
    \item 经过多个编码器层（或解码器层）
    \item 通过输出投影层得到词汇表上的概率分布
\end{enumerate}

\section{关键实现片段}

\subsection{多头自注意力实现}

核心实现代码片段：

\begin{lstlisting}[language=Python, basicstyle=\small]
def scaled_dot_product_attention(self, Q, K, V, mask=None):
    # 计算注意力分数
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
    
    # 应用掩码
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmax归一化
    attention_weights = F.softmax(scores, dim=-1)
    attention_weights = self.dropout(attention_weights)
    
    # 加权求和
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
\end{lstlisting}

\subsection{位置编码实现}

\begin{lstlisting}[language=Python, basicstyle=\small]
def __init__(self, d_model, max_len=5000, dropout=0.1):
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                        (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    self.register_buffer('pe', pe.unsqueeze(0))
\end{lstlisting}

\subsection{编码器层实现}

\begin{lstlisting}[language=Python, basicstyle=\small]
def forward(self, x, mask=None):
    # 多头自注意力 + 残差连接 + 层归一化
    attn_output, _ = self.self_attn(x, x, x, mask)
    x = self.norm1(x + self.dropout1(attn_output))
    
    # 前馈网络 + 残差连接 + 层归一化
    ff_output = self.feed_forward(x)
    x = self.norm2(x + self.dropout2(ff_output))
    
    return x
\end{lstlisting}

\section{实验设置}

\subsection{数据集}

我们使用了一个小规模的中文文本数据集进行训练和验证。数据集包含多个关于修行的故事片段，每行一个文本样本。

\begin{itemize}
    \item 训练集：约100个样本
    \item 验证集：约20个样本
    \item 词汇表大小：根据数据自动构建
\end{itemize}

\subsection{模型配置}

\begin{itemize}
    \item 模型维度（$d_{model}$）：256
    \item 注意力头数（$h$）：8
    \item 编码器层数：4
    \item 前馈网络维度（$d_{ff}$）：1024
    \item 最大序列长度：128
    \item Dropout比率：0.1
\end{itemize}

\subsection{训练配置}

\begin{itemize}
    \item 批次大小：32
    \item 训练轮数：10
    \item 学习率：$1 \times 10^{-4}$
    \item 优化器：AdamW（权重衰减0.01）
    \item 学习率调度：余弦退火
    \item 梯度裁剪阈值：1.0
    \item 随机种子：42
\end{itemize}

\subsection{训练稳定性技术}

为了确保训练稳定，我们实现了以下技术：

\begin{enumerate}
    \item \textbf{梯度裁剪}：防止梯度爆炸
    \item \textbf{学习率调度}：使用余弦退火策略
    \item \textbf{AdamW优化器}：使用权重衰减的正则化
    \item \textbf{Dropout}：防止过拟合
    \item \textbf{层归一化}：稳定训练过程
\end{enumerate}

\section{实验结果}

\subsection{训练过程}

模型在训练过程中，损失函数逐渐下降，表明模型能够学习到数据的模式。训练曲线显示了训练损失和验证损失的变化趋势。

\subsection{模型参数统计}

模型总参数数量取决于词汇表大小和模型配置。在我们的配置下，模型参数数量约为几十万个。

\subsection{结果分析}

\begin{itemize}
    \item 模型能够成功训练，损失函数持续下降
    \item 验证损失与训练损失趋势一致，表明模型没有严重过拟合
    \item 模型能够学习到文本的局部和全局模式
\end{itemize}

\subsection{消融实验建议}

为了进一步验证各组件的有效性，可以进行以下消融实验：

\begin{enumerate}
    \item 不同注意力头数的影响
    \item 不同编码器层数的影响
    \item 有无位置编码的对比
    \item 不同激活函数的影响（ReLU vs GELU）
    \item 不同学习率的影响
\end{enumerate}

\section{总结}

本报告详细介绍了Transformer模型的数学原理和实现细节。我们成功实现了Transformer的核心组件，包括多头自注意力、位置前馈网络、位置编码、残差连接和层归一化。模型在小规模文本数据集上成功训练，损失函数持续下降，验证了实现的正确性。

通过本次作业，我们深入理解了Transformer架构的工作原理，掌握了注意力机制、位置编码等关键技术，为后续学习更复杂的大语言模型奠定了基础。

\section{参考文献}

\begin{enumerate}
    \item Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.
    \item Devlin, J., et al. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
    \item Radford, A., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.
\end{enumerate}

\end{document}

